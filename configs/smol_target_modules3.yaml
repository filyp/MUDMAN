general_config:
  method_name: surgical_irreversible_unlearning
  target_modules:
    - up_proj
  unlearning_loss_fn: correct_logit_minus_avg
  # model_id: EleutherAI/pythia-14m
  model_id: HuggingFaceTB/SmolLM-135M
  retain_set_name: wikitext
  forget_set_name: python
  unlearn_steps: 240
  batch_size: 16
  n_trials: 50
  # method specific
  use_masking: true
  normalize_grads: true
  train_adversary: true
  additional_param_name: null

relearn_config:
  relearn_steps: 120
  relearn_lr: 1.0e-4
  # for Smol I checked that no-LoRA relearns better
  # relearn_lora_conf:
  #   target_modules: all-linear

hyperparams:
  adv_decay: [0.4, 1, false]
  adv_lr: [0.001, 0.006, true]
  fork_every_n_loops: [6, 42, false]
  retain_momentum: [0, 0.99, false]
  retaining_rate: [3.e-4, 3.e-3, true]
  unlearning_rate: [1.e-6, 3.e-4, true]
  # adv_decay: [1, 1, false]  # frozen
  # adv_lr: [0.001, 0.006, true]
  # fork_every_n_loops: [6, 42, false]
  # retain_momentum: [0, 0.99, false]
  # retaining_rate: [1.e-3, 1.e-3, true]  # frozen
  # unlearning_rate: [2.e-5, 1.e-3, true]

variants:

  # done and useful
  all_linear:
    target_modules:
      - up_proj
      - down_proj
      - gate_proj
      - q_proj
      - k_proj
      - v_proj
      - o_proj
  up_proj:
    target_modules:
      - up_proj
  down_proj:
    target_modules:
      - down_proj
  gate_proj:
    target_modules:
      - gate_proj
  q_proj:
    target_modules:
      - q_proj
  k_proj:
    target_modules:
      - k_proj
  v_proj:
    target_modules:
      - v_proj
  o_proj:
    target_modules:
      - o_proj

  gate_v:
    target_modules:
      - gate_proj
      - v_proj
  gate_v_up:
    target_modules:
      - gate_proj
      - v_proj
      - up_proj
  gate_v_up_o:
    target_modules:
      - gate_proj
      - v_proj
      - up_proj
      - o_proj
  gate_v_up_o_q:
    target_modules:
      - gate_proj
      - v_proj
      - up_proj
      - o_proj
      - q_proj


#   # done but not useful
#   # best guess:
#   up_down_gate_o:
#     target_modules:
#       - up_proj
#       - down_
#       - gate_proj
#       - o_proj
#   # additions:
#   up_down_gate_o_q:
#     target_modules:
#       - up_proj
#       - down_proj
#       - gate_proj
#       - o_proj
#       - q_proj
#   up_down_gate_o_k:
#     target_modules:
#       - up_proj
#       - down_proj
#       - gate_proj
#       - o_proj
#       - k_proj
#   up_down_gate_o_v:
#     target_modules:
#       - up_proj
#       - down_proj
#       - gate_proj
#       - o_proj
#       - v_proj
#   # ablations:
#   down_gate_o:
#     target_modules:
#       - down_proj
#       - gate_proj
#       - o_proj
#   up_gate_o:
#     target_modules:
#       - up_proj
#       - gate_proj
#       - o_proj
#   up_down_o:
#     target_modules:
#       - up_proj
#       - down_proj
#       - o_proj
#   up_down_gate:
#     target_modules:
#       - up_proj
#       - down_proj
#       - gate_proj