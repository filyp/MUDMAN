general_config:
  method_name: surgical_irreversible_unlearning
  # target_modules:
  #   - dense_h_to_4h  # equivalent of gate_proj
  target_modules:
    - dense_h_to_4h
    - dense_4h_to_h
    - query_key_value
    - dense
  model_id: EleutherAI/pythia-14m
  retain_set_name: wikitext
  forget_set_name: python
  unlearn_steps: 300
  batch_size: 16
  n_trials: 200
  normalize_grads: true
  retain_loss_budget: 0.05
  # method specific
  unlearning_loss_fn: neg_cross_entropy
  use_masking: true
  train_adversary: true
  additional_param_name: null

relearn_config:
  relearn_steps: 300
  relearn_lr: 1.0e-4

hyperparams:
  additional_param: None  # on default don't use this
  adv_decay: 1  # [0.3, 1, false]
  adv_lr: [0.0003, 0.1, true]
  fork_every_n_loops: 30  # [6, 42, false]
  retain_momentum: [0, 0.99, false]
  retaining_rate: [1.e-4, 1.e-2, true]
  unlearning_rate: [1.e-7, 3.e-2, true]

variants:

  # ! loss functions
  neg_cross_entropy_loss:
    unlearning_loss_fn: neg_cross_entropy
  # neg_entropy_loss:
  #   unlearning_loss_fn: neg_entropy
  # logit_loss:
  #   unlearning_loss_fn: correct_logit_minus_avg

  # ! ablations
  no_masking:
    use_masking: false
  # no_r_momentum:
  #   retain_momentum: [0, 0, false]
  no_adversary:
    train_adversary: false
    # adv_decay: [1, 1, false]  # it has no effect anyway
  # no_adv_decay:
  #   adv_decay: [1, 1, false]
  no_normalization:
    normalize_grads: false
  
  # TAR:
  #   # it also has and target modules
  #   unlearning_loss_fn: neg_entropy
  #   use_masking: false
  #   retain_momentum: [0, 0, false]
  #   additional_param_name: rep_eng_retain_lr
  #   square_norm: false
  #   additional_param: [0, 2, false]
  #   adv_decay: [1, 1, false]
  #   normalize_grads: false
  
  TAR2:
    # it also has and target modules
    # this run uses repE retain loss correctly, with squared norm
    unlearning_loss_fn: neg_entropy
    use_masking: false
    retain_momentum: [0, 0, false]
    additional_param_name: rep_eng_retain_lr
    square_norm: true
    additional_param: [1, 1, false]
    # adv_decay: [1, 1, false]
    normalize_grads: false