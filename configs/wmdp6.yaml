general_config:
  method_name: surgical_irreversible_unlearning
  target_modules:
    - gate_proj
  model_id: meta-llama/Llama-3.2-1B
  retain_set_name: pile_bio_retain
  forget_set_name: pile_bio_forget
  # unlearn_steps: 2400  # 20x what the other runs use
  unlearn_steps: 3600  # 30x what the other runs use
  batch_size: 16
  n_trials: 60
  normalize_grads: true
  # hard_loss_budget: 1000000000
  allowed_mmlu_acc: 0.2923  # base accuarcy is 0.2973, so allow 0.5% drop
  # soft_loss_budget: 0.05
  # method specific
  unlearning_loss_fn: neg_cross_entropy
  use_masking: true
  train_adversary: true
  additional_param_name: null
  eval_wmdp_every: 180 

relearn_config:
  relearn_steps: 1800  # 15x what the other runs use
  relearn_lr: 3.0e-4

hyperparams:
  additional_param: None  # on default don't use this
  adv_decay: 1
  adv_lr: 0.001
  fork_every_n_loops: 48
  retain_momentum: 0.95
  retaining_rate: [3.e-6, 3.e-3, true]
  unlearning_rate: [3.e-9, 1.e-4, true]

variants:

  # ! loss functions
  # neg_cross_entropy_loss:
  #   unlearning_loss_fn: neg_cross_entropy
  neg_entropy_loss:
    unlearning_loss_fn: neg_entropy
  # logit_loss:
  #   unlearning_loss_fn: correct_logit_minus_avg

  # ! ablations
  no_masking_ent:
    use_masking: false
    unlearning_loss_fn: neg_entropy
  no_adversary_ent:
    train_adversary: false
    unlearning_loss_fn: neg_entropy
  no_normalization_ent:
    normalize_grads: false
    unlearning_rate: [3.e-6, 1.e-1, true]
    unlearning_loss_fn: neg_entropy

  # # ! ablations
  # no_masking:
  #   use_masking: false
  #   unlearning_rate: [1.e-8, 1.e-4, true]
  # no_adversary:
  #   train_adversary: false
  # no_normalization:
  #   normalize_grads: false
  #   unlearning_rate: [1.e-2, 1.e+2, true]
  
  TAR2:
    # it also has and target modules
    # this run uses repE retain loss correctly, with squared norm
    unlearning_loss_fn: neg_entropy
    use_masking: false
    retain_momentum: 0
    additional_param_name: rep_eng_retain_lr
    square_norm: true
    additional_param: [1, 1, false]
    normalize_grads: false
    # because it uses no normalization
    unlearning_rate: [3.e-6, 1.e-1, true]