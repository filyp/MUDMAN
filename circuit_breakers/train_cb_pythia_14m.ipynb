{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name_or_path=EleutherAI/pythia-14m\n",
      "user_tag=\n",
      "assistant_tag=\n",
      "output_dir=./out/pythia-14m\n",
      "[2025-01-01 15:45:37,532] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n",
      "15:45:52 INFO  init forget:   3.63    init retain:   5.44\n",
      "[2025-01-01 15:45:52,282] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-01-01 15:45:52,965] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-01-01 15:45:52,965] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "{'target_layers': '3,4', 'transform_layers': '-1', 'lorra_alpha': 5.0, 'trainsets': None, 'valsets': None, 'full_layers': False}\n",
      "LoraArguments(lora_r=16, lora_alpha=16, lora_dropout=0.05, lora_target_modules=['dense'], lora_weight_path='', lora_bias='none', q_lora=False)\n",
      "ModelArguments(model_name_or_path='EleutherAI/pythia-14m', adapter_name_or_path=None, use_lora=False)\n",
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "cache_dir=None,\n",
      "coeff_schedule=linear_converge,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=1000.0,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "grouped_to_max_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_every=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./out/pythia-14m/runs/Jan01_15-45-52_hostname,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.CONSTANT,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=150,\n",
      "metric_for_best_model=None,\n",
      "model_max_length=8192,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./out/pythia-14m,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./out/pythia-14m,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=0,\n",
      "sc_loss_type=orig_act_dotprod,\n",
      "sc_train_seq_type=all_text,\n",
      "sc_train_subset=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=False,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "use_refusal_retain=True,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "lorra_transform_layers [0, 1, 2, 3, 4]\n",
      "drop_layers_after 4\n",
      "15:45:53 INFO  We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Some weights of the model checkpoint at EleutherAI/pythia-14m were not used when initializing GPTNeoXForCausalLM: ['gpt_neox.layers.5.attention.bias', 'gpt_neox.layers.5.attention.dense.bias', 'gpt_neox.layers.5.attention.dense.weight', 'gpt_neox.layers.5.attention.masked_bias', 'gpt_neox.layers.5.attention.query_key_value.bias', 'gpt_neox.layers.5.attention.query_key_value.weight', 'gpt_neox.layers.5.attention.rotary_emb.inv_freq', 'gpt_neox.layers.5.input_layernorm.bias', 'gpt_neox.layers.5.input_layernorm.weight', 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias', 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight', 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias', 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight', 'gpt_neox.layers.5.post_attention_layernorm.bias', 'gpt_neox.layers.5.post_attention_layernorm.weight']\n",
      "- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "['dense'] [0, 1, 2, 3, 4]\n",
      "model PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(50304, 128)\n",
      "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-4): 5 x GPTNeoXLayer(\n",
      "            (input_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (attention): GPTNeoXSdpaAttention(\n",
      "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "              (query_key_value): Linear(in_features=128, out_features=384, bias=True)\n",
      "              (dense): lora.Linear(\n",
      "                (base_layer): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=128, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (mlp): GPTNeoXMLP(\n",
      "              (dense_h_to_4h): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (dense_4h_to_h): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (act): GELUActivation()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "      )\n",
      "      (embed_out): Linear(in_features=128, out_features=50304, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "TRAIN LEN:  1000\n",
      "/home/fgrtr/Code/seek-and-destroy/circuit_breakers/src/lorra_circuit_breaker.py:305: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/fgrtr/Code/seek-and-destroy/circuit_breakers/src/lorra_circuit_breaker.py\", line 378, in <module>\n",
      "[rank0]:     train()\n",
      "[rank0]:   File \"/home/fgrtr/Code/seek-and-destroy/circuit_breakers/src/lorra_circuit_breaker.py\", line 367, in train\n",
      "[rank0]:     trainer.train()\n",
      "[rank0]:   File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/transformers/trainer.py\", line 2164, in train\n",
      "[rank0]:     return inner_training_loop(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/transformers/trainer.py\", line 2524, in _inner_training_loop\n",
      "[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/transformers/trainer.py\", line 3654, in training_step\n",
      "[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: TypeError: train.<locals>.CustomTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
      "[rank0]:[W101 15:45:55.737933228 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "E0101 15:45:56.766000 18352 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 18459) of binary: /home/fgrtr/Code/seek-and-destroy/.venv/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1153, in launch_command\n",
      "    deepspeed_launcher(args)\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 846, in deepspeed_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/fgrtr/Code/seek-and-destroy/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "src/lorra_circuit_breaker.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-01-01_15:45:56\n",
      "  host      : hostname.domain\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 18459)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "# This control which GPU is accessible to this script \n",
    "\n",
    "\n",
    "!./scripts/lorra_circuit_breaker_pythia_14m.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fgrtr/Code/seek-and-destroy/.venv/bin/accelerate\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
