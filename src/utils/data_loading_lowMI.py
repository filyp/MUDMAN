# %%
import json
import re
import shutil
import time
import zipfile
from pathlib import Path

import torch as pt
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer

from datasets import (
    IterableDataset,
    IterableDatasetDict,
    concatenate_datasets,
    load_dataset,
)

base_url = "https://raw.githubusercontent.com/aghyad-deeb/unlearning_evaluation/refs/heads/main/data"
# corpora generated by GPT-4o are corpus_*.jsonl
data_paths = dict(
    wmdp_deduped_unlearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
        "wmdp-deduped/corpus_split_4.jsonl",
    ],
    wmdp_deduped_relearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
    ],
    wmdp_deduped_mcq_eval=[
        "wmdp-deduped/split_4.jsonl",
    ],
    years_unlearning = [
        "dates-years-trimmed/corpus_split_0.jsonl",
        "dates-years-trimmed/corpus_split_1.jsonl",
        "dates-years-trimmed/corpus_split_2.jsonl",
        "dates-years-trimmed/corpus_split_3.jsonl",
        "dates-years-trimmed/corpus_split_4.jsonl",
    ],
    years_relearning = [
        "dates-years-trimmed/corpus_split_0.jsonl",
        "dates-years-trimmed/corpus_split_1.jsonl",
        "dates-years-trimmed/corpus_split_2.jsonl",
        "dates-years-trimmed/corpus_split_3.jsonl",
    ],
    years_mcq_eval = [
        "dates-years-trimmed/split_4.jsonl",
    ],
    # mmlu_forget = [
    #     "mmlu_cats_random_trimmed/corpus_mmlu_STEM.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_business.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_chemistry.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_culture.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_geography.jsonl",
    # ],
    mmlu_retain = [
        "mmlu_cats_random_trimmed/corpus_mmlu_health.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_history.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_law.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_philosophy.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_social sciences.jsonl",
    ],
)

def load_low_mi_set(paths):
    return load_dataset(
        "json", data_files=[f"{base_url}/{path}" for path in paths], split="train"
    )


def load_batches(model_id, paths, batch_size=4, max_length=512):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token

    corpus = load_low_mi_set(paths)
    corpus = corpus.shuffle(seed=42)
    corpus = corpus.batch(batch_size)
    batches = [
        tokenizer(
            x["text"],
            max_length=max_length,
            padding=True,
            truncation=True,
            return_tensors="pt",
        )
        for x in corpus
    ]
    return batches

# # %%
# pt.set_default_device("cuda")
# model_id = "meta-llama/Llama-3.2-1B"
# forget_batches = load_batches(model_id, data_paths["wmdp_deduped_unlearning"])
# retain_batches = load_batches(model_id, data_paths["mmlu_retain"])


# %%

# # %%
# from types import SimpleNamespace

# import sys
# sys.path.append(str(Path(__file__).parent.parent))
# from unlearning_methods.surgical_irreversible_unlearning import surgical_irreversible_unlearning

# # %%
# unlearning_common = SimpleNamespace(
#     model_id = "meta-llama/Llama-3.2-1B",
#     additional_param_name = None,
#     target_modules = [
#         "up_proj",
#         "down_proj",
#         "gate_proj",
#         "q_proj",
#         "k_proj",
#         "v_proj",
#         "o_proj",
#     ],
#     unlearn_steps = 5,
#     train_adversary = True,
#     unlearning_loss_fn = "neg_cross_entropy",
#     use_masking = True,
#     normalize_grads = True,
# )
# h = SimpleNamespace(
#     additional_param = None,
#     adv_decay = 1,
#     adv_lr = 0.001,
#     fork_every_n_loops = 48,
#     retain_momentum = 0.95,
#     retaining_rate = 1e-4,
#     unlearning_rate = 1e-4,
# )

# # start_time = time.time()
# # surgical_irreversible_unlearning(h, config, forget_batches, retain_batches)
# # end_time = time.time()
# # print(f"Time taken: {end_time - start_time} seconds")

# # # %%
# # len(forget_batches)
# %%
