# %%
import json
import re
import shutil
import time
import zipfile
from pathlib import Path

import torch as pt
from huggingface_hub import hf_hub_download
from transformers import AutoTokenizer

from datasets import (
    IterableDataset,
    IterableDatasetDict,
    concatenate_datasets,
    load_dataset,
)

base_url = "https://raw.githubusercontent.com/aghyad-deeb/unlearning_evaluation/refs/heads/main/data"
# corpora generated by GPT-4o are corpus_*.jsonl
data_paths = dict(
    wmdp_deduped_unlearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
        "wmdp-deduped/corpus_split_4.jsonl",
    ],
    wmdp_deduped_relearning=[
        "wmdp-deduped/corpus_split_0.jsonl",
        "wmdp-deduped/corpus_split_1.jsonl",
        "wmdp-deduped/corpus_split_2.jsonl",
        "wmdp-deduped/corpus_split_3.jsonl",
    ],
    wmdp_deduped_mcq_eval=[
        "wmdp-deduped/split_4.jsonl",
    ],
    years_unlearning = [
        "dates-years-trimmed/corpus_split_0.jsonl",
        "dates-years-trimmed/corpus_split_1.jsonl",
        "dates-years-trimmed/corpus_split_2.jsonl",
        "dates-years-trimmed/corpus_split_3.jsonl",
        "dates-years-trimmed/corpus_split_4.jsonl",
    ],
    years_relearning = [
        "dates-years-trimmed/corpus_split_0.jsonl",
        "dates-years-trimmed/corpus_split_1.jsonl",
        "dates-years-trimmed/corpus_split_2.jsonl",
        "dates-years-trimmed/corpus_split_3.jsonl",
    ],
    years_mcq_eval = [
        "dates-years-trimmed/split_4.jsonl",
    ],
    # mmlu_forget = [
    #     "mmlu_cats_random_trimmed/corpus_mmlu_STEM.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_business.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_chemistry.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_culture.jsonl",
    #     "mmlu_cats_random_trimmed/corpus_mmlu_geography.jsonl",
    # ],
    mmlu_retain = [
        "mmlu_cats_random_trimmed/corpus_mmlu_health.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_history.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_law.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_philosophy.jsonl",
        "mmlu_cats_random_trimmed/corpus_mmlu_social sciences.jsonl",
    ],
)

def load_low_mi_set(paths):
    return load_dataset(
        "json", data_files=[f"{base_url}/{path}" for path in paths], split="train"
    )


def load_batches(model_id, dataset_name, batch_size=4, max_length=128):
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token

    paths = data_paths[dataset_name]
    corpus = load_low_mi_set(paths)
    corpus = corpus.shuffle(seed=42)
    corpus = corpus.batch(batch_size)
    batches = [
        tokenizer(
            x["text"],
            max_length=max_length,
            padding=True,
            truncation=True,
            return_tensors="pt",
        )
        for x in corpus
    ]
    return batches

# %%